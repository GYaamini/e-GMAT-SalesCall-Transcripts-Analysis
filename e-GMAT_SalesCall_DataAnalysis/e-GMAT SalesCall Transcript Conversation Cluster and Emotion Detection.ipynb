{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffb8c0-b6a0-453d-a16d-8640c4b114ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as snb\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b224dc-996c-44dc-bc65-37df25c4ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"we need\", \"you need\", \"we should\", \"you should\", \"we will\", \"you will\", \"we have to\", \"you have to\"\n",
    "\n",
    "\"if you\", \"if we\", \"suppose we\", \"we can\", \"you can\", \n",
    "\n",
    "\"is that ok\", \"does it make sense\", \"got it\", \"are we clear\", \"any questions\", \"let me know\", \"anything else\", \"ask me\", \"ping me\", \"contact me\"\n",
    "\n",
    "\"looking for\", \"assistance\", \"support\", \"focus\"\n",
    "\n",
    "\"price point\", \"flexible payment\", \"discount\", \"discounts\"\n",
    "\n",
    "\"send email\", \"send recording\", \"email recording\", \"send study plan\", \"email the resources\", \"send the resources\"\n",
    "\n",
    "\"going to purchase\", \"going to buy\", \"will purchase\", \"will buy\", \"make the purchase\"\n",
    "\n",
    "\"graduate\", \"graduated\", \"studied\"\n",
    "\n",
    "\"working\", \"job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7aca4ef-448f-4da1-ad95-a19cbea9243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import yake\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ceb4a48-715c-4b0e-a5a2-1192cab476ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "# emotion_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "\n",
    "emotion_tokenizer = AutoTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "emotion_model.eval()\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "keyword_extractor = yake.KeywordExtractor(top=8, stopwords=None)\n",
    "# EMOTION_LABELS = ['anger', 'joy', 'optimism', 'sadness']\n",
    "# EMOTION_LABELS = ['eager', 'joy', 'optimism', 'hope', 'anxiety', 'doubt', 'skepticism', 'excitement', 'fear', 'disappointment', 'interest', \n",
    "#                   'awkwardeness', 'confusion', 'confidence', 'relief', 'understanding', 'stress']\n",
    "goemo = load_dataset(\"go_emotions\")\n",
    "EMOTION_LABELS = goemo['train'].features['labels'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42b25b8-d2a5-4960-814c-4c107af30e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_prototypes = {\n",
    "    \"Introduction\": [\"hello\", \"welcome\", \"thanks for joining\", \"good morning\",\n",
    "                    \"graduate\", \"graduated\", \"studied\", \"working\", \"work\", \"job\", \"B.Tech\", \"engineering\", \"M.Tech\", \"MS\"],\n",
    "    \"Prospect’s performance\": [\"last attempt\", \"attempt\", \"your performance\", \"preparation\", \"percentile\", \"mock test\",\n",
    "                               \"targets\", \"achievements\", \"target\", \"goal\", \"aiming\"],\n",
    "    \"Agent drawing up plan\": [\"we’ll create a plan\", \"custom strategy\", \"roadmap\", \"approach\",\n",
    "                             \"looking for\", \"assistance\", \"assist\", \"support\", \"focus\"],\n",
    "    \"Explaining product\": [\"our product\", \"features include\", \"capabilities\", \"what it does\",\n",
    "                          \"we need\", \"you need\", \"we should\", \"you should\", \"we will\", \"you will\", \"we have to\", \"you have to\",\n",
    "                          \"if you\", \"if we\", \"suppose we\", \"we can\", \"you can\",\n",
    "                          \"quant\", \"verbal\", \"insights\", \"DI\", \"score\", \"module\", \"grade\"],\n",
    "    \"Price discussion\": [\"pricing\", \"cost\", \"discount\", \"offer\", \"package\", \n",
    "                         \"price point\", \"flexible payment\", \"discounts\"],\n",
    "    \"Q&A\": [\"any questions\", \"feel free to ask\", \"clarify\", \"follow-up\",\n",
    "           \"is that ok\", \"does it make sense\", \"got it\", \"are we clear\", \"let me know\", \"anything else\", \"ask me\", \"ping me\", \"contact me\"],\n",
    "    \"Wrap\": [\"going to purchase\", \"going to buy\", \"will purchase\", \"will buy\", \"make the purchase\", \n",
    "             \"send email\", \"send recording\", \"email recording\", \"send study plan\", \"email the resources\", \"send the resources\"]\n",
    "}\n",
    "prototype_texts = list(phase_prototypes.keys())\n",
    "prototype_embeddings = embedding_model.encode([\" \".join(v) for v in phase_prototypes.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5edc8177-017c-4158-b94a-4709194dcfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotions(text):\n",
    "    inputs = emotion_tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        logits = emotion_model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits)[0]\n",
    "    top_idxs = (probs > 0.3).nonzero(as_tuple=True)[0]  # threshold\n",
    "    emotions = [(EMOTION_LABELS[i], probs[i].item()) for i in top_idxs]\n",
    "    return emotions\n",
    "\n",
    "def extract_keywords(text):\n",
    "    return [kw for kw, _ in keyword_extractor.extract_keywords(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e85f3-079f-471c-8575-7ca9dbdf7233",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29311363-1c6b-4f86-a678-14cd8c2aa68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"transcripts.csv\").dropna(subset=[\"text\"])\n",
    "df = df.sort_values(by=[\"transcript_id\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd61953-4abf-42a1-9a17-65e0a659d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f4d10d-188e-43a0-b60c-e26690ffd893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transcript_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100_20250213</th>\n",
       "      <td>00:00:02</td>\n",
       "      <td>01:17:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101_20250212</th>\n",
       "      <td>00:00:02</td>\n",
       "      <td>00:58:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102_20250212</th>\n",
       "      <td>00:00:04</td>\n",
       "      <td>00:58:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103_20250212</th>\n",
       "      <td>00:00:03</td>\n",
       "      <td>01:06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104_20250212</th>\n",
       "      <td>00:00:05</td>\n",
       "      <td>00:41:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96_20250211</th>\n",
       "      <td>00:00:02</td>\n",
       "      <td>01:03:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97_20250212</th>\n",
       "      <td>00:00:03</td>\n",
       "      <td>00:53:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98_20250212</th>\n",
       "      <td>00:00:02</td>\n",
       "      <td>00:24:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99_20250211</th>\n",
       "      <td>00:00:02</td>\n",
       "      <td>00:40:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9_20250106</th>\n",
       "      <td>00:00:02</td>\n",
       "      <td>00:29:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    min       max\n",
       "transcript_id                    \n",
       "100_20250213   00:00:02  01:17:17\n",
       "101_20250212   00:00:02  00:58:49\n",
       "102_20250212   00:00:04  00:58:29\n",
       "103_20250212   00:00:03  01:06:40\n",
       "104_20250212   00:00:05  00:41:18\n",
       "...                 ...       ...\n",
       "96_20250211    00:00:02  01:03:27\n",
       "97_20250212    00:00:03  00:53:29\n",
       "98_20250212    00:00:02  00:24:49\n",
       "99_20250211    00:00:02  00:40:38\n",
       "9_20250106     00:00:02  00:29:21\n",
       "\n",
       "[142 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_ranges = df.groupby('transcript_id')['timestamp'].agg(['min', 'max'])\n",
    "timestamp_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e1ceb99-a9d2-402c-bced-c1d435602261",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION_PERCENTAGES = {\n",
    "    1: 0.05,   # 5% Introduction\n",
    "    2: 0.15,   # 15% Prospect’s performance\n",
    "    3: 0.20,   # 20% Agent drawing up plan\n",
    "    4: 0.35,   # 35% Explaining product\n",
    "    5: 0.10,   # 10% Price discussion\n",
    "    6: 0.10,   # 10% Q&A\n",
    "    7: 0.05    # 5% Wrap\n",
    "}\n",
    "\n",
    "assert sum(SECTION_PERCENTAGES.values()) == 1.0, \"Percentages must sum to 100%\"\n",
    "\n",
    "def assign_sections(group):\n",
    "    group = group.sort_values('timestamp')\n",
    "    total_rows = len(group)\n",
    "    \n",
    "    cumulative_rows = 0\n",
    "    group['section_number'] = np.nan\n",
    "    \n",
    "    for section, pct in SECTION_PERCENTAGES.items():\n",
    "        start_row = cumulative_rows\n",
    "        end_row = cumulative_rows + int(round(total_rows * pct))\n",
    "        \n",
    "        # Handle last section to include remaining rows (due to rounding)\n",
    "        if section == 7:\n",
    "            end_row = total_rows\n",
    "        \n",
    "        # section numbers assignment\n",
    "        group.iloc[start_row:end_row, group.columns.get_loc('section_number')] = section\n",
    "        cumulative_rows = end_row\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31c63e10-45fb-413a-8d0e-5f728520686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yaamini\\AppData\\Local\\Temp\\ipykernel_6160\\2079218042.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sections = df.groupby('transcript_id', group_keys=False).apply(assign_sections)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>section_number</th>\n",
       "      <th>start_timestamp</th>\n",
       "      <th>end_timestamp</th>\n",
       "      <th>row_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100_20250213</td>\n",
       "      <td>1.0</td>\n",
       "      <td>00:00:02</td>\n",
       "      <td>00:04:02</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100_20250213</td>\n",
       "      <td>2.0</td>\n",
       "      <td>00:04:16</td>\n",
       "      <td>00:15:26</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100_20250213</td>\n",
       "      <td>3.0</td>\n",
       "      <td>00:15:29</td>\n",
       "      <td>00:32:19</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100_20250213</td>\n",
       "      <td>4.0</td>\n",
       "      <td>00:32:24</td>\n",
       "      <td>01:00:46</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100_20250213</td>\n",
       "      <td>5.0</td>\n",
       "      <td>01:00:52</td>\n",
       "      <td>01:08:05</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>9_20250106</td>\n",
       "      <td>3.0</td>\n",
       "      <td>00:05:45</td>\n",
       "      <td>00:11:46</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>9_20250106</td>\n",
       "      <td>4.0</td>\n",
       "      <td>00:11:51</td>\n",
       "      <td>00:22:10</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>9_20250106</td>\n",
       "      <td>5.0</td>\n",
       "      <td>00:22:11</td>\n",
       "      <td>00:25:17</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>9_20250106</td>\n",
       "      <td>6.0</td>\n",
       "      <td>00:25:21</td>\n",
       "      <td>00:28:14</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>9_20250106</td>\n",
       "      <td>7.0</td>\n",
       "      <td>00:28:18</td>\n",
       "      <td>00:29:21</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>994 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    transcript_id  section_number start_timestamp end_timestamp  row_count\n",
       "0    100_20250213             1.0        00:00:02      00:04:02         42\n",
       "1    100_20250213             2.0        00:04:16      00:15:26        127\n",
       "2    100_20250213             3.0        00:15:29      00:32:19        169\n",
       "3    100_20250213             4.0        00:32:24      01:00:46        296\n",
       "4    100_20250213             5.0        01:00:52      01:08:05         85\n",
       "..            ...             ...             ...           ...        ...\n",
       "989    9_20250106             3.0        00:05:45      00:11:46        105\n",
       "990    9_20250106             4.0        00:11:51      00:22:10        183\n",
       "991    9_20250106             5.0        00:22:11      00:25:17         52\n",
       "992    9_20250106             6.0        00:25:21      00:28:14         52\n",
       "993    9_20250106             7.0        00:28:18      00:29:21         27\n",
       "\n",
       "[994 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sections = df.groupby('transcript_id', group_keys=False).apply(assign_sections)\n",
    "\n",
    "section_ranges = (df_sections.groupby(['transcript_id', 'section_number'])\n",
    "    .agg(start_timestamp=('timestamp', 'min'),end_timestamp=('timestamp', 'max'),row_count=('timestamp', 'count')).reset_index()\n",
    ")\n",
    "section_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "660708e5-956b-4058-af76-f77322189f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_ranges.to_csv(\"sections_in_transcripts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fe01ab7-ca86-49af-9f1b-e1464db239ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_speaker</th>\n",
       "      <th>datetime</th>\n",
       "      <th>section_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feb</td>\n",
       "      <td>100_20250213</td>\n",
       "      <td>00:00:02</td>\n",
       "      <td>Matt, so basically I have completed my B.Tech ...</td>\n",
       "      <td>prospect</td>\n",
       "      <td>2025-05-29 00:00:02</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feb</td>\n",
       "      <td>100_20250213</td>\n",
       "      <td>00:00:14</td>\n",
       "      <td>OK.</td>\n",
       "      <td>agent</td>\n",
       "      <td>2025-05-29 00:00:14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feb</td>\n",
       "      <td>100_20250213</td>\n",
       "      <td>00:00:14</td>\n",
       "      <td>So after that I have joined PwC India and I ha...</td>\n",
       "      <td>prospect</td>\n",
       "      <td>2025-05-29 00:00:14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feb</td>\n",
       "      <td>100_20250213</td>\n",
       "      <td>00:00:33</td>\n",
       "      <td>OK.</td>\n",
       "      <td>agent</td>\n",
       "      <td>2025-05-29 00:00:33</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feb</td>\n",
       "      <td>100_20250213</td>\n",
       "      <td>00:00:34</td>\n",
       "      <td>It was the reason I will tell you it was that ...</td>\n",
       "      <td>prospect</td>\n",
       "      <td>2025-05-29 00:00:34</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79671</th>\n",
       "      <td>Jan</td>\n",
       "      <td>9_20250106</td>\n",
       "      <td>00:29:18</td>\n",
       "      <td>Yeah, thank you.</td>\n",
       "      <td>prospect</td>\n",
       "      <td>2025-05-29 00:29:18</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79672</th>\n",
       "      <td>Jan</td>\n",
       "      <td>9_20250106</td>\n",
       "      <td>00:29:20</td>\n",
       "      <td>Thank you.</td>\n",
       "      <td>prospect</td>\n",
       "      <td>2025-05-29 00:29:20</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79673</th>\n",
       "      <td>Jan</td>\n",
       "      <td>9_20250106</td>\n",
       "      <td>00:29:20</td>\n",
       "      <td>Bye.</td>\n",
       "      <td>agent</td>\n",
       "      <td>2025-05-29 00:29:20</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79674</th>\n",
       "      <td>Jan</td>\n",
       "      <td>9_20250106</td>\n",
       "      <td>00:29:20</td>\n",
       "      <td>Bye.</td>\n",
       "      <td>agent</td>\n",
       "      <td>2025-05-29 00:29:20</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79675</th>\n",
       "      <td>Jan</td>\n",
       "      <td>9_20250106</td>\n",
       "      <td>00:29:21</td>\n",
       "      <td>Bye.</td>\n",
       "      <td>agent</td>\n",
       "      <td>2025-05-29 00:29:21</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79676 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      month transcript_id timestamp  \\\n",
       "0       Feb  100_20250213  00:00:02   \n",
       "1       Feb  100_20250213  00:00:14   \n",
       "2       Feb  100_20250213  00:00:14   \n",
       "3       Feb  100_20250213  00:00:33   \n",
       "4       Feb  100_20250213  00:00:34   \n",
       "...     ...           ...       ...   \n",
       "79671   Jan    9_20250106  00:29:18   \n",
       "79672   Jan    9_20250106  00:29:20   \n",
       "79673   Jan    9_20250106  00:29:20   \n",
       "79674   Jan    9_20250106  00:29:20   \n",
       "79675   Jan    9_20250106  00:29:21   \n",
       "\n",
       "                                                    text predicted_speaker  \\\n",
       "0      Matt, so basically I have completed my B.Tech ...          prospect   \n",
       "1                                                    OK.             agent   \n",
       "2      So after that I have joined PwC India and I ha...          prospect   \n",
       "3                                                    OK.             agent   \n",
       "4      It was the reason I will tell you it was that ...          prospect   \n",
       "...                                                  ...               ...   \n",
       "79671                                   Yeah, thank you.          prospect   \n",
       "79672                                         Thank you.          prospect   \n",
       "79673                                               Bye.             agent   \n",
       "79674                                               Bye.             agent   \n",
       "79675                                               Bye.             agent   \n",
       "\n",
       "                 datetime  section_number  \n",
       "0     2025-05-29 00:00:02             1.0  \n",
       "1     2025-05-29 00:00:14             1.0  \n",
       "2     2025-05-29 00:00:14             1.0  \n",
       "3     2025-05-29 00:00:33             1.0  \n",
       "4     2025-05-29 00:00:34             1.0  \n",
       "...                   ...             ...  \n",
       "79671 2025-05-29 00:29:18             7.0  \n",
       "79672 2025-05-29 00:29:20             7.0  \n",
       "79673 2025-05-29 00:29:20             7.0  \n",
       "79674 2025-05-29 00:29:20             7.0  \n",
       "79675 2025-05-29 00:29:21             7.0  \n",
       "\n",
       "[79676 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf8699-4fc1-4cca-9ed3-bf81169d21e4",
   "metadata": {},
   "source": [
    "## Clustering and Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "773f3d92-008b-4632-90ce-6c4e0ddcb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "EDUCATION_KEYWORDS = [\n",
    "    \"high school\", \"diploma\", \"bachelor\", \"bachelors\", \"b.sc\", \"b.sc.\", \"bcom\", \"b.com\", \"ba\",\n",
    "    \"b.a\", \"b.e\", \"b.tech\", \"m.sc\", \"m.sc.\", \"mca\", \"m.tech\", \"m.com\", \"phd\", \"ph.d\",\n",
    "    \"doctorate\", \"masters\", \"graduate\", \"postgraduate\", \"undergraduate\", \"engineering\", \n",
    "    \"commerce\", \"arts\", \"science\", \"computer science\", \"information technology\"\n",
    "]\n",
    "\n",
    "def extract_education_from_text(text, keywords=EDUCATION_KEYWORDS):\n",
    "    text = text.lower()\n",
    "    matches = []\n",
    "    for keyword in keywords:\n",
    "        # Use word boundaries to avoid partial matches (e.g., \"mba\" in \"embarrass\")\n",
    "        if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
    "            matches.append(keyword)\n",
    "    return list(set(matches))\n",
    "\n",
    "def extract_ner_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {\n",
    "        \"location\": set(),\n",
    "        \"organization\": set(),\n",
    "        \"person\": set(),\n",
    "        \"age\": set(),\n",
    "        \"date\": set(),\n",
    "        \"education\": []\n",
    "    }\n",
    "\n",
    "    entities['education'] = extract_education_from_text(text)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "            entities[\"location\"].add(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            entities[\"organization\"].add(ent.text)\n",
    "        elif ent.label_ == \"PERSON\":\n",
    "            entities[\"person\"].add(ent.text)\n",
    "        elif ent.label_ == \"DATE\":\n",
    "            if any(char.isdigit() for char in ent.text):\n",
    "                entities[\"date\"].add(ent.text)\n",
    "        elif ent.label_ == \"AGE\":\n",
    "            entities[\"age\"].add(ent.text)\n",
    "\n",
    "    return {k: list(v) for k, v in entities.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9baad2b3-1c78-4d75-bcdf-508a6c741822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100_20250213\n",
      "101_20250212\n",
      "102_20250212\n",
      "103_20250212\n",
      "104_20250212\n",
      "105_20250215\n",
      "106_20250215\n",
      "107_20250215\n",
      "108_20250216\n",
      "109_20250216\n",
      "10_20250107\n",
      "110_20250216\n",
      "111_20250216\n",
      "112_20250213\n",
      "113_20250217\n",
      "114_20250214\n",
      "115_20250214\n",
      "116_20250217\n",
      "117_20250217\n",
      "118_20250217\n",
      "119_20250217\n",
      "11_20250107\n",
      "120_20250218\n",
      "121_20250218\n",
      "122_20250219\n",
      "123_20250219\n",
      "124_20250219\n",
      "125_20250221\n",
      "126_20250221\n",
      "127_20250222\n",
      "128_20250223\n",
      "129_20250223\n",
      "12_20250107\n",
      "130_20250219\n",
      "131_20250224\n",
      "132_20250224\n",
      "133_20250225\n",
      "134_20250226\n",
      "135_20250226\n",
      "136_20250226\n",
      "137_20250227\n",
      "138_20250226\n",
      "139_20250227\n",
      "13_20250108\n",
      "140_20250227\n",
      "141_20250228\n",
      "142_20250224\n",
      "14_20250110\n",
      "15_20250110\n",
      "16_20250110\n",
      "17_20250110\n",
      "18_20250111\n",
      "19_20250112\n",
      "1_20250101\n",
      "20_20250112\n",
      "21_20250112\n",
      "22_20250112\n",
      "23_20250112\n",
      "24_20250111\n",
      "25_20250110\n",
      "26_20250113\n",
      "27_20250110\n",
      "28_20250113\n",
      "29_20250113\n",
      "2_20250102\n",
      "30_20250114\n",
      "31_20250113\n",
      "32_20250113\n",
      "33_20250114\n",
      "34_20250114\n",
      "35_20250115\n",
      "36_20250114\n",
      "37_20250115\n",
      "38_20250115\n",
      "39_20250116\n",
      "3_20250102\n",
      "40_20250116\n",
      "41_20250117\n",
      "42_20250119\n",
      "43_20250119\n",
      "44_20250119\n",
      "45_20250119\n",
      "46_20250117\n",
      "47_20250120\n",
      "48_20250120\n",
      "49_20250116\n",
      "4_20250103\n",
      "50_20250121\n",
      "51_20250121\n",
      "52_20250121\n",
      "53_20250122\n",
      "54_20250122\n",
      "55_20250122\n",
      "56_20250121\n",
      "57_20250122\n",
      "58_20250120\n",
      "59_20250121\n",
      "5_20250104\n",
      "60_20250124\n",
      "61_20250124\n",
      "62_20250124\n",
      "63_20250127\n",
      "64_20250127\n",
      "65_20250127\n",
      "66_20250128\n",
      "67_20250128\n",
      "68_20250129\n",
      "69_20250130\n",
      "6_20250104\n",
      "70_20250131\n",
      "71_20250129\n",
      "72_20250129\n",
      "73_20250130\n",
      "74_20250202\n",
      "75_20250202\n",
      "76_20250202\n",
      "77_20250203\n",
      "78_20250204\n",
      "79_20250204\n",
      "7_20250105\n",
      "80_20250204\n",
      "81_20250205\n",
      "82_20250205\n",
      "83_20250205\n",
      "84_20250206\n",
      "85_20250206\n",
      "86_20250207\n",
      "87_20250207\n",
      "88_20250207\n",
      "89_20250208\n",
      "8_20250105\n",
      "90_20250208\n",
      "91_20250208\n",
      "92_20250208\n",
      "93_20250210\n",
      "94_20250210\n",
      "95_20250211\n",
      "96_20250211\n",
      "97_20250212\n",
      "98_20250212\n",
      "99_20250211\n",
      "9_20250106\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "sections_text = []\n",
    "count = 0\n",
    "\n",
    "for tid, group in df_sections.groupby(\"transcript_id\"):\n",
    "    print(tid)\n",
    "    group = group.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    section_data = []\n",
    "    section_text_data = []\n",
    "    ner_info = {}\n",
    "\n",
    "    for section_id in sorted(group['section_number'].unique()):\n",
    "        phase = prototype_texts[int(section_id) - 1]\n",
    "        chunk = group[group[\"section_number\"] == section_id]\n",
    "        chunk_texts = chunk[\"text\"].tolist()\n",
    "        chunk_speakers = chunk[\"predicted_speaker\"].tolist()\n",
    "\n",
    "        agent_texts = [txt for txt, spk in zip(chunk_texts, chunk_speakers) if spk == \"agent\"]\n",
    "        prospect_texts = [txt for txt, spk in zip(chunk_texts, chunk_speakers) if spk == \"prospect\"]\n",
    "\n",
    "        agent_blob = \" \".join(agent_texts)\n",
    "        prospect_blob = \" \".join(prospect_texts)\n",
    "\n",
    "        if int(section_id) == 1:\n",
    "            ner_info = extract_ner_entities(prospect_blob)\n",
    "\n",
    "        agent_emotions_predicted = predict_emotions(agent_blob)\n",
    "        agent_emotions = [e[0] for e in agent_emotions_predicted]\n",
    "        agent_scores = [e[1] for e in agent_emotions_predicted]\n",
    "\n",
    "        prospect_emotions_predicted = predict_emotions(prospect_blob)\n",
    "        prospect_emotions = [e[0] for e in prospect_emotions_predicted]\n",
    "        prospect_scores = [e[1] for e in prospect_emotions_predicted]\n",
    "\n",
    "        agent_keywords = extract_keywords(agent_blob)\n",
    "        prospect_keywords = extract_keywords(prospect_blob)\n",
    "\n",
    "        section_data.append({\n",
    "            \"section_number\": int(section_id),\n",
    "            \"phase\": phase,\n",
    "            \"speaker emotion\": agent_emotions,\n",
    "            \"speaker emotion_score\": agent_scores,\n",
    "            \"speaker keywords\": \", \".join(agent_keywords),\n",
    "            \"speaker duration\": len(agent_texts),\n",
    "            \"prospect emotion\": prospect_emotions,\n",
    "            \"prospect emotion_score\": prospect_scores,\n",
    "            \"prospect keywords\": \", \".join(prospect_keywords),\n",
    "            \"prospect duration\": len(prospect_texts),\n",
    "            \"start_timestamp\": chunk[\"timestamp\"].iloc[0],\n",
    "            \"end_timestamp\": chunk[\"timestamp\"].iloc[-1]\n",
    "        })\n",
    "\n",
    "        section_text_data.append({\n",
    "            \"section_number\": int(section_id),\n",
    "            \"phase\": phase,\n",
    "            \"agent_text\": agent_blob,\n",
    "            \"prospect_text\": prospect_blob\n",
    "        })\n",
    "\n",
    "    results.append({\n",
    "        \"transcript_id\": tid,\n",
    "        \"month\": group['month'].iloc[0],\n",
    "        \"sections\": section_data\n",
    "    })\n",
    "    for ner,val in ner_info.items():\n",
    "        results[count][ner] = val\n",
    "    count += 1\n",
    "\n",
    "    sections_text.append({\n",
    "        \"transcript_id\": tid,\n",
    "        \"sections\": section_text_data,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a02fca1e-26de-43a0-b629-a1a6f1dfcd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcript_analysis_sections.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "with open(\"transcript_sections_texts.json\", \"w\") as f:\n",
    "    json.dump(sections_text, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb44ee-9687-40e6-9e0e-0bf48106e3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
